  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What steps of the plan were performed and what steps were skipped (explain why)?\n",
    "\n",
    "I performed all the core steps outlined in the initial project plan, including loading and merging the four datasets (contract, personal, internet, and phone) using customerID as the key to create a unified dataframe. I then handled data preprocessing by addressing missing values (e.g., filling categoricals with 'No' and converting TotalCharges to numeric with a flag for missing billing), engineering features like Tenure (days since contract start), one-hot encoding categoricals, and scaling numerical features. For modeling, I started with a Random Forest baseline, then switched to XGBoost for better performance on the imbalanced dataset, using GridSearchCV to tune hyperparameters and evaluate on a stratified train-test split. Finally, I retrained the model on the full dataset and saved it for deployment. No major steps were skipped, though I initially considered using SMOTE for oversampling but opted for XGBoost's built-in scale_pos_weight instead, as it simplified the process without needing additional libraries and achieved excellent results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What difficulties did you encounter and how did you manage to solve them?\n",
    "\n",
    "One difficulty was installing the imbalanced-learn library for SMOTE, which failed due to permission issues in the environment. I solved this by using XGBoost's scale_pos_weight parameter instead, which effectively handled the class imbalance (churn vs. no-churn) without external dependencies. Another challenge was dealing with the TotalCharges column, which contained non-numeric values and empty strings. I resolved this by converting it to numeric with error handling, creating a 'MissingBilling' flag for missing rows, and filling with zero, ensuring no data loss. Additionally, I initially had data leakage issues by preprocessing after the train-test split; I fixed this by moving all preprocessing (encoding and scaling) before splitting, ensuring the model only used training data for fitting transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What were some of the key steps to solving the task?\n",
    "\n",
    "The key steps included merging the datasets to consolidate all features, thorough preprocessing to handle missing values and create useful features like Tenure, and selecting XGBoost with hyperparameter tuning to maximize AUC-ROC. Feature engineering (e.g., Tenure and MissingBilling) was crucial for capturing churn patterns, while stratified splitting maintained class balance. Evaluating on unseen test data and iterating based on metrics ensured reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is your final model and what quality score does it have?\n",
    "\n",
    "My final model is an XGBoost classifier with the following tuned hyperparameters: learning_rate=0.2, max_depth=7, n_estimators=200, and scale_pos_weightâ‰ˆ3.77 (to address imbalance). It achieved an AUC-ROC of 0.9385 and an accuracy of 0.8893 on the test set, indicating strong performance in predicting churn. This exceeds the project threshold of 0.88 AUC-ROC, qualifying for the maximum score.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
